{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Region Visual Cropping for Qwen2.5-VL (Improved)\n",
    "\n",
    "This notebook extends the single-region ViCrop method to support multiple regions.\n",
    "\n",
    "## Key Improvements over Naive Approach\n",
    "1. **Peak Detection**: Uses `peak_local_max` instead of greedy masking\n",
    "2. **Gaussian Suppression**: Smooth region separation instead of binary masks\n",
    "3. **IoU-based Overlap Control**: Properly prevents overlapping regions\n",
    "4. **Attention Threshold**: Filters out low-quality regions\n",
    "5. **Quality Scoring**: Combined metric of attention density + sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "# Import improved multi-region utilities\n",
    "from multi_region_utils import (\n",
    "    bbox_from_att_multi_region,\n",
    "    crop_multi_regions,\n",
    "    adaptive_num_regions,\n",
    "    filter_regions_by_quality,\n",
    "    visualize_regions,\n",
    "    is_relational_or_counting_question\n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Qwen/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    ").eval().to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_path, \n",
    "    trust_remote_code=True, \n",
    "    padding_side='left', \n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_base64(image):\n",
    "    \"\"\"Encodes a PIL image to a base64 string.\"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return img_str\n",
    "\n",
    "def prepare_qwen2_5_input(messages, processor):\n",
    "    \"\"\"Prepare the input for Qwen2.5VL.\"\"\"\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Attention Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_attention(image, question, model, processor, att_layer=22):\n",
    "    \"\"\"\n",
    "    Compute relative attention for a given image and question.\n",
    "    \n",
    "    The relative attention normalizes question-specific attention by \n",
    "    general description attention to highlight semantically relevant regions.\n",
    "    \"\"\"\n",
    "    image_str = encode_base64(image)\n",
    "    \n",
    "    # Question-specific attention\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [\n",
    "             {\"type\": \"image\", \"image\": f'data:image;base64,{image_str}'},\n",
    "             {\"type\": \"text\", \"text\": f\"{question} Answer the question using a single word or phrase.\"}\n",
    "         ]}\n",
    "    ]\n",
    "    \n",
    "    # General description for normalization\n",
    "    general_messages = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [\n",
    "             {\"type\": \"image\", \"image\": f'data:image;base64,{image_str}'},\n",
    "             {\"type\": \"text\", \"text\": \"Write a general description of the image. Answer the question using a single word or phrase.\"}\n",
    "         ]}\n",
    "    ]\n",
    "    \n",
    "    inputs = prepare_qwen2_5_input(messages, processor).to(model.device, torch.bfloat16)\n",
    "    general_inputs = prepare_qwen2_5_input(general_messages, processor).to(model.device, torch.bfloat16)\n",
    "    \n",
    "    att_shape = (inputs['image_grid_thw'][0, 1:] / 2).cpu().numpy().astype(int).tolist()\n",
    "    \n",
    "    vision_start_token_id = processor.tokenizer.convert_tokens_to_ids('<|vision_start|>')\n",
    "    vision_end_token_id = processor.tokenizer.convert_tokens_to_ids('<|vision_end|>')\n",
    "    \n",
    "    pos = inputs['input_ids'].tolist()[0].index(vision_start_token_id) + 1\n",
    "    pos_end = inputs['input_ids'].tolist()[0].index(vision_end_token_id)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "        general_outputs = model(**general_inputs, output_attentions=True)\n",
    "    \n",
    "    att = outputs['attentions'][att_layer][0, :, -1, pos:pos_end].mean(dim=0).to(torch.float32).detach().cpu().numpy()\n",
    "    general_att = general_outputs['attentions'][att_layer][0, :, -1, pos:pos_end].mean(dim=0).to(torch.float32).detach().cpu().numpy()\n",
    "    \n",
    "    # Relative attention with epsilon to avoid division by zero\n",
    "    att_map = att / (general_att + 1e-8)\n",
    "    att_map = att_map.reshape(att_shape)\n",
    "    \n",
    "    return att_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Region VQA Inference (Improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_region_vqa(\n",
    "    image, \n",
    "    question, \n",
    "    model, \n",
    "    processor, \n",
    "    num_regions=None,  # None = auto-detect based on question\n",
    "    max_overlap_iou=0.3,\n",
    "    min_attention_ratio=0.15,\n",
    "    min_score_ratio=0.4,\n",
    "    visualize=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform VQA with improved multi-region cropping.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        question: Question string\n",
    "        model: Qwen2.5-VL model\n",
    "        processor: Qwen2.5-VL processor\n",
    "        num_regions: Number of regions (None = auto-detect)\n",
    "        max_overlap_iou: Maximum IoU between regions\n",
    "        min_attention_ratio: Minimum attention to consider a region\n",
    "        min_score_ratio: Minimum score ratio for region filtering\n",
    "        visualize: Whether to visualize results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains answer, bboxes, crops, scores, attention_map\n",
    "    \"\"\"\n",
    "    # Auto-detect number of regions based on question type\n",
    "    if num_regions is None:\n",
    "        num_regions = adaptive_num_regions(question, default=1)\n",
    "        is_rel, is_count, keywords = is_relational_or_counting_question(question)\n",
    "        if is_rel or is_count:\n",
    "            print(f\"Detected {'counting' if is_count else 'relational'} question (keywords: {keywords})\")\n",
    "            print(f\"Using {num_regions} regions\")\n",
    "    \n",
    "    # Compute attention map\n",
    "    att_map = compute_relative_attention(image, question, model, processor)\n",
    "    \n",
    "    # Get multiple bounding boxes with scores\n",
    "    bboxes, scores = bbox_from_att_multi_region(\n",
    "        att_map, \n",
    "        image.size, \n",
    "        num_regions=num_regions,\n",
    "        max_overlap_iou=max_overlap_iou,\n",
    "        min_attention_ratio=min_attention_ratio\n",
    "    )\n",
    "    \n",
    "    # Filter by quality score\n",
    "    bboxes, scores = filter_regions_by_quality(\n",
    "        bboxes, scores, \n",
    "        min_score_ratio=min_score_ratio\n",
    "    )\n",
    "    \n",
    "    print(f\"Selected {len(bboxes)} regions with scores: {[f'{s:.3f}' for s in scores]}\")\n",
    "    \n",
    "    # Crop regions\n",
    "    crops = crop_multi_regions(image, bboxes)\n",
    "    \n",
    "    # Visualize if requested\n",
    "    if visualize:\n",
    "        fig = visualize_regions(image, bboxes, att_map)\n",
    "        plt.show()\n",
    "    \n",
    "    # Prepare multi-image input: [original, crop1, crop2, ...]\n",
    "    image_strs = [encode_base64(image)] + [encode_base64(crop) for crop in crops]\n",
    "    \n",
    "    content = []\n",
    "    for img_str in image_strs:\n",
    "        content.append({\"type\": \"image\", \"image\": f'data:image;base64,{img_str}'})\n",
    "    content.append({\"type\": \"text\", \"text\": f\"{question} Answer the question using a single word or phrase.\"})\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    \n",
    "    inputs = prepare_qwen2_5_input(messages, processor).to(model.device, torch.bfloat16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    answer = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'bboxes': bboxes,\n",
    "        'crops': crops,\n",
    "        'scores': scores,\n",
    "        'attention_map': att_map,\n",
    "        'num_regions_used': len(bboxes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline VQA (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vqa(image, question, model, processor):\n",
    "    \"\"\"\n",
    "    Perform VQA without any cropping (baseline).\n",
    "    \"\"\"\n",
    "    image_str = encode_base64(image)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": [\n",
    "             {\"type\": \"image\", \"image\": f'data:image;base64,{image_str}'},\n",
    "             {\"type\": \"text\", \"text\": f\"{question} Answer the question using a single word or phrase.\"}\n",
    "         ]}\n",
    "    ]\n",
    "    \n",
    "    inputs = prepare_qwen2_5_input(messages, processor).to(model.device, torch.bfloat16)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    answer = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image\n",
    "image_path = './images/demo1.png'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Test Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Relational question\n",
    "question = \"What is to the left of the person?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n[Baseline - No Cropping]\")\n",
    "baseline_answer = baseline_vqa(image, question, model, processor)\n",
    "print(f\"Answer: {baseline_answer}\")\n",
    "\n",
    "# Multi-region (auto-detect)\n",
    "print(\"\\n[Multi-Region Cropping - Auto]\")\n",
    "result = multi_region_vqa(image, question, model, processor, visualize=True)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Counting question\n",
    "question = \"How many people are in the image?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n[Baseline - No Cropping]\")\n",
    "baseline_answer = baseline_vqa(image, question, model, processor)\n",
    "print(f\"Answer: {baseline_answer}\")\n",
    "\n",
    "# Multi-region (auto-detect)\n",
    "print(\"\\n[Multi-Region Cropping - Auto]\")\n",
    "result = multi_region_vqa(image, question, model, processor, visualize=True)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Detail question (should use single region)\n",
    "question = \"What text is visible on the sign?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline\n",
    "print(\"\\n[Baseline - No Cropping]\")\n",
    "baseline_answer = baseline_vqa(image, question, model, processor)\n",
    "print(f\"Answer: {baseline_answer}\")\n",
    "\n",
    "# Multi-region (auto-detect - should use 1 region)\n",
    "print(\"\\n[Multi-Region Cropping - Auto]\")\n",
    "result = multi_region_vqa(image, question, model, processor, visualize=True)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different parameters\n",
    "question = \"What is between the two people?\"\n",
    "\n",
    "print(\"Testing different overlap thresholds...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for max_iou in [0.1, 0.3, 0.5]:\n",
    "    print(f\"\\nmax_overlap_iou = {max_iou}\")\n",
    "    result = multi_region_vqa(\n",
    "        image, question, model, processor,\n",
    "        num_regions=3,\n",
    "        max_overlap_iou=max_iou,\n",
    "        visualize=False\n",
    "    )\n",
    "    print(f\"  Regions: {len(result['bboxes'])}, Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(image, questions, visualize=False):\n",
    "    \"\"\"\n",
    "    Compare baseline vs multi-region cropping on multiple questions.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        baseline = baseline_vqa(image, question, model, processor)\n",
    "        multi = multi_region_vqa(image, question, model, processor, visualize=visualize)\n",
    "        \n",
    "        print(f\"Baseline:     {baseline}\")\n",
    "        print(f\"Multi-Region: {multi['answer']} ({multi['num_regions_used']} regions)\")\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'baseline': baseline,\n",
    "            'multi_region': multi['answer'],\n",
    "            'num_regions': multi['num_regions_used']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison on multiple question types\n",
    "test_questions = [\n",
    "    # Relational\n",
    "    \"What is to the left of the person?\",\n",
    "    \"What is to the right of the person?\",\n",
    "    \"What is between the objects?\",\n",
    "    # Counting\n",
    "    \"How many people are in the image?\",\n",
    "    \"How many objects are on the table?\",\n",
    "    # Detail (single region)\n",
    "    \"What color is the shirt?\",\n",
    "    \"What text is shown?\",\n",
    "]\n",
    "\n",
    "results = compare_methods(image, test_questions, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The improved multi-region implementation addresses the following issues:\n",
    "\n",
    "1. **Peak Detection**: Uses `skimage.feature.peak_local_max` to find true local maxima in the attention map, rather than greedily masking regions.\n",
    "\n",
    "2. **Gaussian Suppression**: After selecting a region, applies smooth Gaussian suppression instead of binary masking, allowing nearby regions to still contribute.\n",
    "\n",
    "3. **IoU Control**: Actually implements overlap checking using Intersection over Union (the original code defined `min_overlap` but never used it).\n",
    "\n",
    "4. **Quality Thresholding**: Filters out low-quality regions based on both attention threshold and score ratio.\n",
    "\n",
    "5. **Adaptive Regions**: Automatically detects question type (counting/relational/detail) and adjusts the number of regions accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
